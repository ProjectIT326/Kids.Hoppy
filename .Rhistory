library(readxl)
Hobby_Data <- read_excel("Hobby_Data.xlsx")
View(Hobby_Data)
Hobby_Data$Olympiad_Participation = factor(Hobby_Data$Olympiad_Participation,levels = c("No", "Yes"), labels = c(0, 1))
Hobby_Data$Scholarship = factor(Hobby_Data$Scholarship , levels = c("No", "Yes"), labels = c(0, 1))
Hobby_Data$School = factor(Hobby_Data$School, levels = c("No", "Yes"), labels = c(0, 1))
Hobby_Data$Projects = factor(Hobby_Data$Projects, levels = c("No", "Yes"), labels = c(0, 1))
Hobby_Data$Medals = factor(Hobby_Data$Medals, levels = c("No", "Yes"), labels = c(0, 1))
Hobby_Data$Career_sprt = factor(Hobby_Data$Career_sprt, levels = c("No", "Yes"), labels = c(0, 1))
Hobby_Data$Act_sprt = factor(Hobby_Data$Act_sprt, levels = c("No", "Yes"), labels = c(0, 1))
Hobby_Data$Fant_arts = factor(Hobby_Data$Fant_arts, levels = c("No", "Yes"), labels = c(0, 1))
Hobby_Data$Won_arts = factor(Hobby_Data$Won_arts, levels = c("No", "Maybe", "Yes"), labels = c(0, 2, 1))
Hobby_Data$`Predicted Hobby` <- factor(Hobby_Data$`Predicted Hobby`, levels = c("Academics", "Arts", "Sports"), labels = c(1, 2, 3))
Hobby_Data <- Hobby_Data[, !(colnames(Hobby_Data) %in% c("School", "Medals"))]
# Load the required packages
library(rpart)
library(rpart.plot)
library(caret)
# Set the seed for reproducibility
set.seed(1234)
# Split the data into training and testing sets
ind <- sample(2, nrow(Hobby_Data), replace=TRUE, prob=c(0.7, 0.3))
trainData <- Hobby_Data[ind == 1,]
testData <- Hobby_Data[ind == 2,]
# Define the formula for the decision tree
myFormula <- `Predicted Hobby` ~ Scholarship + Fav_sub + Projects + Grasp_pow + Time_sprt + Career_sprt + Act_sprt + Fant_arts + Won_arts + Time_art + Olympiad_Participation
# Create the decision tree model with the "information" splitting criterion
Hobby_Data_ctree <- rpart(myFormula, data = trainData, method = "class", parms = list(split = "information"))
# Print the decision tree
print(Hobby_Data_ctree)
# Plot the decision tree
rpart.plot(Hobby_Data_ctree)
# Predict on the test data
testPred <- predict(Hobby_Data_ctree, newdata = testData,type = 'class' )
# Check the accuracy of the model
accuracy <- sum(testPred == testData$`Predicted Hobby`) / nrow(testData) * 100
accuracy
# Calculate the confusion matrix and display results
results <- confusionMatrix(testPred, testData$`Predicted Hobby`)
print(results)
#-----------------------------------------------------------------------
# Define individual class values for each metric
sensitivity_class_1 <- 0.9241
sensitivity_class_2 <- 0.8654
sensitivity_class_3 <- 0.7805
specificity_class_1 <- 0.8602
specificity_class_2 <- 0.9750
specificity_class_3 <- 0.9542
pos_pred_value_class_1 <- 0.8488
pos_pred_value_class_2 <- 0.9375
pos_pred_value_class_3 <- 0.8421
neg_pred_value_class_1 <- 0.9302
neg_pred_value_class_2 <- 0.9435
neg_pred_value_class_3 <- 0.9328
prevalence_class_1 <- 0.4593
prevalence_class_2 <- 0.3023
prevalence_class_3 <- 0.2384
detection_rate_class_1 <- 0.4244
detection_rate_class_2 <- 0.2616
detection_rate_class_3 <- 0.1860
balanced_accuracy_class_1 <- 0.8921
balanced_accuracy_class_2 <- 0.9202
balanced_accuracy_class_3 <- 0.8673
# Calculate macro-averages for each metric
macro_avg_sensitivity <- (sensitivity_class_1 + sensitivity_class_2 + sensitivity_class_3) / 3
macro_avg_specificity <- (specificity_class_1 + specificity_class_2 + specificity_class_3) / 3
macro_avg_pos_pred_value <- (pos_pred_value_class_1 + pos_pred_value_class_2 + pos_pred_value_class_3) / 3
macro_avg_neg_pred_value <- (neg_pred_value_class_1 + neg_pred_value_class_2 + neg_pred_value_class_3) / 3
macro_avg_prevalence <- (prevalence_class_1 + prevalence_class_2 + prevalence_class_3) / 3
macro_avg_detection_rate <- (detection_rate_class_1 + detection_rate_class_2 + detection_rate_class_3) / 3
macro_avg_balanced_accuracy <- (balanced_accuracy_class_1 + balanced_accuracy_class_2 + balanced_accuracy_class_3) / 3
# Print the macro-averages for each metric
print(paste('Average Sensitivity(Recall):', macro_avg_sensitivity))
print(paste('Average Specificity:', macro_avg_specificity))
print(paste('Average Pos Pred Value:', macro_avg_pos_pred_value))
print(paste('Average Neg Pred Value:', macro_avg_neg_pred_value))
print(paste('Average Prevalence:', macro_avg_prevalence))
print(paste('Average Detection Rate:', macro_avg_detection_rate))
print(paste('Average Balanced Accuracy:', macro_avg_balanced_accuracy))
library(rpart)
library(caTools)
library(rpart.plot)
library(caret)
# Set the seed for reproducibility
set.seed(1234)
# Split the data into training and testing sets
ind <- sample(2, nrow(Hobby_Data), replace=TRUE, prob=c(0.8, 0.2))
trainData <- Hobby_Data[ind == 1,]
testData <- Hobby_Data[ind == 2,]
# Define the formula for the decision tree
myFormula <- `Predicted Hobby` ~ Scholarship + Fav_sub + Projects + Grasp_pow + Time_sprt + Career_sprt + Act_sprt + Fant_arts + Won_arts + Time_art + Olympiad_Participation
# Create the decision tree model with the "information" splitting criterion
Hobby_Data_ctree <- rpart(myFormula, data = trainData, method = "class", parms = list(split = "information"))
# Print the decision tree
print(Hobby_Data_ctree)
# Plot the decision tree
rpart.plot(Hobby_Data_ctree)
# Predict on the test data
testPred <- predict(Hobby_Data_ctree, newdata = testData,type = 'class' )
# Check the accuracy of the model
accuracy <- sum(testPred == testData$`Predicted Hobby`) / nrow(testData) * 100
accuracy
# Calculate the confusion matrix and display results
results <- confusionMatrix(testPred, testData$`Predicted Hobby`)
print(results)
#---------------------------------------------------------------------------
# Define individual class values for each metric
sensitivity_class_1 <- 0.9241
sensitivity_class_2 <- 0.8654
sensitivity_class_3 <- 0.7805
specificity_class_1 <- 0.8602
specificity_class_2 <- 0.9750
specificity_class_3 <- 0.9542
pos_pred_value_class_1 <- 0.8488
pos_pred_value_class_2 <- 0.9375
pos_pred_value_class_3 <- 0.8421
neg_pred_value_class_1 <- 0.9302
neg_pred_value_class_2 <- 0.9435
neg_pred_value_class_3 <- 0.9328
prevalence_class_1 <- 0.4593
prevalence_class_2 <- 0.3023
prevalence_class_3 <- 0.2384
detection_rate_class_1 <- 0.4244
detection_rate_class_2 <- 0.2616
detection_rate_class_3 <- 0.1860
balanced_accuracy_class_1 <- 0.8921
balanced_accuracy_class_2 <- 0.9202
balanced_accuracy_class_3 <- 0.8673
# Calculate macro-averages for each metric
macro_avg_sensitivity <- (sensitivity_class_1 + sensitivity_class_2 + sensitivity_class_3) / 3
macro_avg_specificity <- (specificity_class_1 + specificity_class_2 + specificity_class_3) / 3
macro_avg_pos_pred_value <- (pos_pred_value_class_1 + pos_pred_value_class_2 + pos_pred_value_class_3) / 3
macro_avg_neg_pred_value <- (neg_pred_value_class_1 + neg_pred_value_class_2 + neg_pred_value_class_3) / 3
macro_avg_prevalence <- (prevalence_class_1 + prevalence_class_2 + prevalence_class_3) / 3
macro_avg_detection_rate <- (detection_rate_class_1 + detection_rate_class_2 + detection_rate_class_3) / 3
macro_avg_balanced_accuracy <- (balanced_accuracy_class_1 + balanced_accuracy_class_2 + balanced_accuracy_class_3) / 3
# Print the macro-averages for each metric
print(paste('Average Sensitivity(Recall):', macro_avg_sensitivity))
print(paste('Average Specificity:', macro_avg_specificity))
print(paste('Average Pos Pred Value:', macro_avg_pos_pred_value))
print(paste('Average Neg Pred Value:', macro_avg_neg_pred_value))
print(paste('Average Prevalence:', macro_avg_prevalence))
print(paste('Average Detection Rate:', macro_avg_detection_rate))
print(paste('Average Balanced Accuracy:', macro_avg_balanced_accuracy))
library(rpart)
library(caTools)
library(rpart.plot)
library(caret)
# Set the seed for reproducibility
set.seed(1234)
# Split the data into training and testing sets
ind <- sample(2, nrow(Hobby_Data), replace=TRUE, prob=c(0.9, 0.1))
trainData <- Hobby_Data[ind == 1,]
testData <- Hobby_Data[ind == 2,]
# Define the formula for the decision tree
myFormula <- `Predicted Hobby` ~ Scholarship + Fav_sub + Projects + Grasp_pow + Time_sprt + Career_sprt + Act_sprt + Fant_arts + Won_arts + Time_art + Olympiad_Participation
# Create the decision tree model with the "information" splitting criterion
Hobby_Data_ctree <- rpart(myFormula, data = trainData, method = "class", parms = list(split = "information"))
# Print the decision tree
print(Hobby_Data_ctree)
# Plot the decision tree
rpart.plot(Hobby_Data_ctree)
# Predict on the test data
testPred <- predict(Hobby_Data_ctree, newdata = testData,type = 'class' )
# Check the accuracy of the model
accuracy <- sum(testPred == testData$`Predicted Hobby`) / nrow(testData) * 100
accuracy
# Calculate the confusion matrix and display results
results <- confusionMatrix(testPred, testData$`Predicted Hobby`)
print(results)
#----------------------------------------------------------------------------------
# Define individual class values for each metric
sensitivity_class_1 <- 0.9241
sensitivity_class_2 <- 0.8654
sensitivity_class_3 <- 0.7805
specificity_class_1 <- 0.8602
specificity_class_2 <- 0.9750
specificity_class_3 <- 0.9542
pos_pred_value_class_1 <- 0.8488
pos_pred_value_class_2 <- 0.9375
pos_pred_value_class_3 <- 0.8421
neg_pred_value_class_1 <- 0.9302
neg_pred_value_class_2 <- 0.9435
neg_pred_value_class_3 <- 0.9328
prevalence_class_1 <- 0.4593
prevalence_class_2 <- 0.3023
prevalence_class_3 <- 0.2384
detection_rate_class_1 <- 0.4244
detection_rate_class_2 <- 0.2616
detection_rate_class_3 <- 0.1860
balanced_accuracy_class_1 <- 0.8921
balanced_accuracy_class_2 <- 0.9202
balanced_accuracy_class_3 <- 0.8673
# Calculate macro-averages for each metric
macro_avg_sensitivity <- (sensitivity_class_1 + sensitivity_class_2 + sensitivity_class_3) / 3
macro_avg_specificity <- (specificity_class_1 + specificity_class_2 + specificity_class_3) / 3
macro_avg_pos_pred_value <- (pos_pred_value_class_1 + pos_pred_value_class_2 + pos_pred_value_class_3) / 3
macro_avg_neg_pred_value <- (neg_pred_value_class_1 + neg_pred_value_class_2 + neg_pred_value_class_3) / 3
macro_avg_prevalence <- (prevalence_class_1 + prevalence_class_2 + prevalence_class_3) / 3
macro_avg_detection_rate <- (detection_rate_class_1 + detection_rate_class_2 + detection_rate_class_3) / 3
macro_avg_balanced_accuracy <- (balanced_accuracy_class_1 + balanced_accuracy_class_2 + balanced_accuracy_class_3) / 3
# Print the macro-averages for each metric
print(paste('Average Sensitivity(Recall):', macro_avg_sensitivity))
print(paste('Average Specificity:', macro_avg_specificity))
print(paste('Average Pos Pred Value:', macro_avg_pos_pred_value))
print(paste('Average Neg Pred Value:', macro_avg_neg_pred_value))
print(paste('Average Prevalence:', macro_avg_prevalence))
print(paste('Average Detection Rate:', macro_avg_detection_rate))
print(paste('Average Balanced Accuracy:', macro_avg_balanced_accuracy))
