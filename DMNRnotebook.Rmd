---
title: "R Notebook"
output: html_notebook
---
## **Kids Hobby Prediction Dataset**

**Description:**

Following the selection of our data set ("Hobby_Data") which predicts kids' hobbies, that was collected by asking their parents specific questions about their kid's preferences, capabilities, and achievements. To help us train the machine to predict the kid's hobby. We will begin to preprocess and analyze the data.

**General information about the data set:**

```{r}
str(Hobby_Data)
``````

**summary of the dataset:**

- samples of raw dataset:

```
{r}
sample(Hobby_Data)
```



 Statistical measures :
```{r}
summary (Hobby_Data$Grasp_pow)

mode(Hobby_Data$Grasp_pow)
mean(Hobby_Data$Grasp_pow)

mean(Hobby_Data$Time_sprt)

summary(Hobby_Data$Time_sprt)


summary(Hobby_Data$Time_art)
mode(Hobby_Data$Time_art)

Mode <- function(Hobby_Data) {
  ux <- unique(Hobby_Data)
  ux[which.max(tabulate(match(Hobby_Data, ux)))]
} 

modes<-Mode(Hobby_Data)


str( modes)



 

```
#1# Data cleaning

During the data cleaning stage, Finding and fixing faults, inconsistencies, and errors in a dataset helps it be more reliable and of higher quality for analysis and modelling. There are methods for handling missing values, detecting outliers, resolving inconsistencies, and standardising formats.

-   import Dataset"Hobby_Data"

```{r}
setwd("/Users/96653/Documents/GitHub/KidsHobby/Dataset")



View(Hobby_Data)

str(Hobby_Data)
```

**1- check missing value :**

```{r}
is.na(Hobby_Data)
```

-   **find the total null values in the dataset**

```{r}
sum(is.na(Hobby_Data))
```

We simply look for missing values and no missing values in their ,According to our investigation, the dataset does not contain any outliers , since numric Data Type is not included. also there are no inconsistent values,or other errors.

#Encoding:
#Converting categorical or non-numeric data into a numerical format, which is necessary for compatibility with machine learning 
```{r}
Hobby_Data$Olympiad_Participation = factor(Hobby_Data$Olympiad_Participation,levels = c("No", "Yes"), labels = c(0, 1))
Hobby_Data$Scholarship = factor(Hobby_Data$Scholarship , levels = c("No", "Yes"), labels = c(0, 1))
Hobby_Data$School = factor(Hobby_Data$School, levels = c("No", "Yes"), labels = c(0, 1))
Hobby_Data$Projects = factor(Hobby_Data$Projects, levels = c("No", "Yes"), labels = c(0, 1))
Hobby_Data$Medals = factor(Hobby_Data$Medals, levels = c("No", "Yes"), labels = c(0, 1))
Hobby_Data$Career_sprt = factor(Hobby_Data$Career_sprt, levels = c("No", "Yes"), labels = c(0, 1))
Hobby_Data$Act_sprt = factor(Hobby_Data$Act_sprt, levels = c("No", "Yes"), labels = c(0, 1))
Hobby_Data$Fant_arts = factor(Hobby_Data$Fant_arts, levels = c("No", "Yes"), labels = c(0, 1))
Hobby_Data$Won_arts = factor(Hobby_Data$Won_arts, levels = c("No", "Maybe", "Yes"), labels = c(0, 2, 1))
Hobby_Data$Fav_sub = factor(Hobby_Data$Fav_sub, levels = c("Science", "Mathematics", "History/Geography", "Any language"), labels = c(1, 2, 3, 4))
```

#Normalization and Discetization
#We don't need to use normalization and discetization in our dataset.Since our dataset doesnâ€™t have numeric attributes and normalization involves mathematical operations, which can result in meaningless and erroneous values,Also, applying discretization leads to loss of information and makes intervals and relationships that don't exist between values.



Feature Selection
Feature selection is the process of selecting a subset of relevant features from a larger set of available features. The main objective of feature selection is to improve the performance and efficiency of machine learning models. By selecting the most informative and influential features while discarding irrelevant or redundant ones to improves model performance.

1-Rank Features By Importance
The provided code performs a sequence of steps to analyze the importance of different features selection in predicting kids hobbies using a Random Forest model.

# Ensure the results are repeatable by setting a seed
```{r}
set.seed(7)

```


# Load the necessary libraries
```{r}
install.packages("caret")
install.packages("randomForest")
library(caret)
library(randomForest)
```



# Load the dataset
```{r}
data(Hobby_Data)
```


# Convert the class label to a factor
```{r}
Hobby_Data$`Predicted Hobby` <- as.factor(Hobby_Data$`Predicted Hobby`)
```


# Separate the predictors and the class label
```{r}
predictors <- Hobby_Data[, -14]  # Excluding the class label (Predicted Hobby)
class_label <- Hobby_Data$`Predicted Hobby`
```

# Train a Random Forest model
```{r}
model <- randomForest(predictors, class_label, importance = TRUE)
```


# Get the variable importance
```{r}
importance <- importance(model)
```


# Rank the features by importance
```{r}
ranked_features <- sort(importance[, "MeanDecreaseGini"], decreasing = TRUE)
```


# Print the ranked features
```{r}
print(ranked_features)

barplot(ranked_features, horiz = TRUE, las = 1, main = "Kids Hobby Variable Importance Ranking")
```



#----------------------------------------------------
2-Feature Selection Using RFE
The provided code aims to identify the most important features for predicting kids hobbies using Recursive Feature Elimination (RFE) .

# Ensure the results are repeatable by setting a seed
```{r}
set.seed(7)
```

# Load the necessary libraries
```{r}
library(caret)
```


# Load the data
```{r}
data(Hobby_Data)
```


# Define the control parameters for RFE using random forest selection function
```{r}
control <- rfeControl(functions = rfFuncs, method = "cv", number = 10)
```


# Extract the predictor variables from Hobby_Data
```{r}
predictors <- Hobby_Data[, -ncol(Hobby_Data)]
```


# Convert the outcome variable to a factor
```{r}
outcome <- as.factor(Hobby_Data$`Predicted Hobby`)
```


# Run the RFE algorithm
```{r}
results <- rfe(predictors, outcome, sizes = 1:ncol(Hobby_Data), rfeControl = control)
```


# Summarize the results
```{r}
print(results)
```



# List the chosen features selected by RFE
```{r}
predictors(results)
```


# Plot the results
```{r}
plot(results, type = c("g", "o"))
```



---------------------------------------
By considering both Recursive Feature Elimination (RFE) and Rank By Importance, we can make informed decisions about feature relevance and impact on the model. In this case, the columns "School," "Medals"  should be deleted as they have lower importance scores compared to the selected variables. Removing these columns simplifies the model and reduces dimensionality, eliminating potential noise and irrelevant information that could hinder accurate predictions.

# Remove the specified columns from the Hobby_Kids dataset
```{r}
Hobby_Data <- Hobby_Data[, !(colnames(Hobby_Data) %in% c("School", "Medals"))]
```

# Display the updated dataset after deleting columns
```{r}
View(Hobby_Data)
```
