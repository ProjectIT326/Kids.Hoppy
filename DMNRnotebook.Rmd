---
title: "R Notebook"
output: html_notebook
---

## **Kids Hobby Prediction Dataset**
```{r}
summary(Hobby_Data)
```

**Description:**
Following the selection of our data set ("Hobby_Data") which predicts kids' hobbies, that was collected by asking their parents specific questions about their kid's preferences, capabilities, and achievements. To help us train the machine to predict the kid's hobby. We will begin to preprocess and analyze the data.

**Dataset source:**
https://www.kaggle.com/datasets/abtabm/hobby-prediction-basic

**Attribute** **Description:**

| Attribute name         | **Description**                                               | **Data type** |
|------------------------|---------------------------------------------------------------|---------------|
| Olympiad_Participation | Has your child participated in any Science/Maths              | Boolean       |
| Scholarship            | Has he/she received any scholarship?                          | Boolean       |
| School                 | Love's going to school?                                       | Boolean       |
| Fav_sub                | What is his/her favorite subject?                             | Categorical   |
| Projects               | Has done any projects under academics before?                 | Boolean       |
| Grasp_pow              | His/Her Grasping power (1-6)                                  | Ordinal       |
| Time_sprt              | How much time does he/she spend playing outdoor/indoor games? | Ordinal       |
| Medals                 | Medals won in Sports?                                         | Boolean       |
| Career_sprt            | Want's to pursue his/her career in sports?                    | Boolean       |
| Act_sprt               | Regular in his/her sports activities?                         | Boolean       |
| Fant_arts              | Love creating fantasy paintings?                              | Boolean       |
| Won_arts               | Won art competitions?                                         | Ordinal       |
| Time_art               | Time utilized in Arts?                                        | Ordinal       |
| Predicted Hobby        | predictions for the hobby that the kid wouldl ike             | Categorical   |

**General information about the data set:**

```{r}
str(Hobby_Data)
```

**summary of the dataset:**

-   **samples of raw dataset:**

```{r}
sample(Hobby_Data)
```

-   **variables distribution:**

    In our dataset, numeric variables are not available; instead, we have three ordinal variables. Due to the nature of our data types, certain types of graphs, such as scatter plots and box plots, were not suitable for our analysis.

variables distribution of Time_sprt:

```{r}
install.packages("magrittr") # install only one time then put this command as comment after installation
library(magrittr) ## for pipe operations
Hobby_Data$Time_art %>% density() %>% plot(main='variables distribution of Time_art')

```

In the "Time_art" variable, parents were requested to assess the time their child dedicated to artistic pursuits like painting or paper crafting, using a scale ranging from 1 to 6, where 6 represents the highest level of involvement. It's worth noting that the concentration of lower ratings at the lower end of the scale (1) is quite pronounced, and this tendency may be attributed to the inherent inclination of children towards physical activities.

variables distribution of Time_art:

```{r}
Hobby_Data$Time_sprt %>% density() %>% plot(main='variables distribution of Time_sprt')
```

Parents were requested to assess their children's involvement in sports on a scale from 1 to 6 within the "Time_sprt" variable. Notably, the most prevalent ranking was 3, suggesting a moderate level of sports participation. It's interesting to observe that the distribution exhibits a shape akin to a bell curve, indicating that a substantial proportion of children have a genuine love for sports.

variables distribution of Grasp_pow:

```{r}
Hobby_Data$Grasp_pow %>% density() %>% plot(main='variables distribution of Grasp_pow')
```

The density graph depicting parents' ratings of their children's grasp power, which ranges from 1 to 6, illustrates a trend where the most common rating is level 3, followed by level 4, level 5, level 2, level 1, and level 6. This distribution indicates that a substantial proportion of parents believe their children possess grasp power that is average or slightly above average (levels 3 and 4), with fewer children rated at the extremes (levels 1, 2, 5, and 6).

variables distribution of the class label 'Predicted Hobby':

```{r}
install.packages("dplyr") # install only one time then put this command as comment after installation
library(dplyr)

dataset2 <- Hobby_Data %>% sample_n(1600)
table(dataset2$`Predicted Hobby`) %>% pie()
tab <- dataset2$`Predicted Hobby` %>% table()
precentages <- tab %>% prop.table() %>% round(3) * 100 
txt <- paste0(names(tab), '\n', precentages, '%')
pie(tab, labels=txt)
```

The pie chart illustrates the distribution of the class label 'Predicted Hobby'. It's evident that a substantial portion, approximately 43.7%, of the children's hobbies are academic in nature, indicating a strong interest in educational pursuits. Additionally, 30.8% of the kids are engaged in sports, reflecting a significant inclination towards physical activities. Arts-related hobbies account for 25.6% of the total, suggesting a considerable creative and artistic engagement among the children. This distribution provides insights into the diverse interests and activities that engage the young population.

**#1# Data cleaning:**

During the data cleaning stage, finding and fixing faults, inconsistencies, and errors in a dataset helps it be more reliable and of higher quality for analysis and modeling. There are methods for handling missing values, detecting outliers, resolving inconsistencies, and standardizing formats.

import Dataset"Hobby_Data"

```{r}
setwd("/Users/96653/Documents/GitHub/KidsHobby/Dataset")



View(Hobby_Data)

str(Hobby_Data)
```

check missing value :

```{r}
is.na(Hobby_Data)
```

find the total null values in the dataset:

```{r}
sum(is.na(Hobby_Data))
```

We simply looked for missing values, and there are no missing values in the dataset. According to our investigation, the dataset does not contain any outliers since it doesn't have a numerical data type. Additionally, there are no inconsistent values or other errors.

**#2#Encoding:**

Converting categorical or non-numeric data into a numerical format, which is necessary for compatibility with machine learning

```{r}
Hobby_Data$Olympiad_Participation = factor(Hobby_Data$Olympiad_Participation,levels = c("No", "Yes"), labels = c(0, 1))
Hobby_Data$Scholarship = factor(Hobby_Data$Scholarship , levels = c("No", "Yes"), labels = c(0, 1))
Hobby_Data$School = factor(Hobby_Data$School, levels = c("No", "Yes"), labels = c(0, 1))
Hobby_Data$Projects = factor(Hobby_Data$Projects, levels = c("No", "Yes"), labels = c(0, 1))
Hobby_Data$Medals = factor(Hobby_Data$Medals, levels = c("No", "Yes"), labels = c(0, 1))
Hobby_Data$Career_sprt = factor(Hobby_Data$Career_sprt, levels = c("No", "Yes"), labels = c(0, 1))
Hobby_Data$Act_sprt = factor(Hobby_Data$Act_sprt, levels = c("No", "Yes"), labels = c(0, 1))
Hobby_Data$Fant_arts = factor(Hobby_Data$Fant_arts, levels = c("No", "Yes"), labels = c(0, 1))
Hobby_Data$Won_arts = factor(Hobby_Data$Won_arts, levels = c("No", "Maybe", "Yes"), labels = c(0, 2, 1))
Hobby_Data$Fav_sub = factor(Hobby_Data$Fav_sub, levels = c("Science", "Mathematics", "History/Geography", "Any language"), labels = c(1, 2, 3, 4))
```

**Statistical measures :**

```{r}
find_mode <- function(x) {
  u <- unique(x)
  tab <- tabulate(match(x, u))
  u[tab == max(tab)]
}

find_mode(Hobby_Data$Time_art)
<<<<<<< Updated upstream
hist(Hobby_Data$Time_art)
=======
>>>>>>> Stashed changes
```

The histogram makes it evident that the mode is equal to 1, and we believe that the high frequency of parents ranking "1" as the most chosen rank in the "Time_art" variable could be attributed to several factors. It might indicate that a significant number of parents perceive their children's involvement in art activities as relatively low, possibly due to time constraints, academic priorities, or a limited interest in art. Alternatively, it could reflect that parents value a more balanced approach to their children's activities, with a variety of interests and responsibilities sharing their time. This trend could also result from a cultural or educational emphasis on other subjects and extracurricular activities that compete for a child's time, potentially leading to a lower ranking for art-related activities.

```{r}
find_mode(Hobby_Data$Grasp_pow)
hist(Hobby_Data$Grasp_pow)
```

Rank 3, in this context, may have been the most chosen rank because it likely represents an average or moderate level of grasp power. Parents may have assessed their children's grasp power as neither exceptionally strong (rank 5 or 6) nor particularly weak (rank 1 or 2), resulting in the preference for the middle-ranking option. This choice could reflect a perception that their children's grasp power falls within a typical or expected range, making it the most common rating.

```{r}
find_mode(Hobby_Data$Time_sprt)
hist(Hobby_Data$Time_sprt)
```

As shown in the histogram the rank 3 on a scale of 1 to 6 was likely the most chosen rank for assessing children's involvement in sports because it represents a balanced middle ground. Parents may have perceived rank 3 as indicating that their children are moderately involved in sports, not excessively committed or disinterested. This middle-of-the-road ranking reflects a common perspective that many parents may hold, considering that extreme rankings, such as 1 or 6, might suggest either a lack of involvement or an excessive focus on sports, which may not align with their perception of their child's overall well-rounded development.

The histogram graph for "Time_art" variable
```{r}
hist(Hobby_Data$Time_art)
```

The histogram makes it evident that the mode is equal to 1, and we believe that the high frequency of parents ranking "1" as the most chosen rank in the "Time_art" variable could be attributed to several factors. It might indicate that a significant number of parents perceive their children's involvement in art activities as relatively low, possibly due to time constraints, academic priorities, or a limited interest in art. Alternatively, it could reflect that parents value a more balanced approach to their children's activities, with a variety of interests and responsibilities sharing their time. This trend could also result from a cultural or educational emphasis on other subjects and extracurricular activities that compete for a child's time, potentially leading to a lower ranking for art-related activities.


```{r}
find_mode(Hobby_Data$Grasp_pow)
```
The histogram graph for "Grasp_pow" variable
```{r}
hist(Hobby_Data$Grasp_pow)
```
Rank 3, in this context, may have been the most chosen rank because it likely represents an average or moderate level of grasp power. Parents may have assessed their children's grasp power as neither exceptionally strong (rank 5 or 6) nor particularly weak (rank 1 or 2), resulting in the preference for the middle-ranking option. This choice could reflect a perception that their children's grasp power falls within a typical or expected range, making it the most common rating.


```{r}
find_mode(Hobby_Data$Time_sprt)
```

The histogram graph for "Time_sprt" variable
```{r}
hist(Hobby_Data$Time_sprt)
```
As shown in the histogram the rank 3 on a scale of 1 to 6 was likely the most chosen rank for assessing children's involvement in sports because it represents a balanced middle ground. Parents may have perceived rank 3 as indicating that their children are moderately involved in sports, not excessively committed or disinterested. This middle-of-the-road ranking reflects a common perspective that many parents may hold, considering that extreme rankings, such as 1 or 6, might suggest either a lack of involvement or an excessive focus on sports, which may not align with their perception of their child's overall well-rounded development.


**#3# Normalization and Discetization:**

We don't need to use normalization and discetization in our dataset. Since our dataset doesn't have numeric attributes and normalization involves mathematical operations, which can result in meaningless values and errors, Also, applying discretization leads to a loss of information and creates intervals and relationships that don't exist between values.

**#4#Feature Selection:**

To improve the accuracy of our predictions for the target class "Predicted Hobby" and decrease the processing time of our classifier, we will utilize feature selection techniques. These techniques enable us to eliminate redundant or irrelevant attributes from the dataset, resulting in a more concise subset of features that provide the most valuable information for our predictions.

Specifically, we will use two feature selection methods: Rank Features by Importance and Feature Selection Using Recursive Feature Elimination (RFE).

1.  **Rank Features by Importance:**

This method used in the code helps us determine which features are most important for predicting the "Predicted Hobby" class label in the dataset. It utilizes the Random Forest algorithm, known for its accurate prediction capabilities. This method calculates the importance of each feature by assessing its contribution to the overall accuracy of the predictions. By ranking the features based on their importance, we can identify the ones that have the greatest influence on determining hobbies.

Ensure the results are repeatable by setting a seed:

```{r}
set.seed(7)

```

Load the necessary libraries:

```{r}
install.packages("caret")
install.packages("randomForest")
library(caret)
library(randomForest)
```

Convert the class label to a factor:

```{r}
Hobby_Data$`Predicted Hobby` <- as.factor(Hobby_Data$`Predicted Hobby`)
```

Separate the predictors and the class label:

```{r}
predictors <- Hobby_Data[, -14]  # Excluding the class label (Predicted Hobby)
class_label <- Hobby_Data$`Predicted Hobby`
```

Train a Random Forest model:

```{r}
model <- randomForest(predictors, class_label, importance = TRUE)
```

Get the variable importance:

```{r}
importance <- importance(model)
```

Rank the features by importance:

```{r}
ranked_features <- sort(importance[, "MeanDecreaseGini"], decreasing = TRUE)
```

Print the ranked features:

```{r}
print(ranked_features)

barplot(ranked_features, horiz = TRUE, las = 1, main = "Kids Hobby Variable Importance Ranking")
```

2.  **Feature Selection Using RFE:**

The Recursive Feature Elimination (RFE) method with Random Forest is a technique used to select the most important features for accurate predictions . It iteratively eliminates less relevant features, retraining the model at each step to evaluate performance. By focusing on the most informative features, RFE improves reduces complexity, and enhances prediction accuracy. It is particularly effective with Random Forest due to its ability to handle complex relationships and high-dimensional data. RFE helps identify the most important attributes associated with the target variable, enabling the creation of more efficient and accurate models.

Ensure the results are repeatable by setting a seed:

```{r}
set.seed(7)
```

Load the necessary libraries:

```{r}
library(caret)
```

Define the control parameters for RFE using random forest selection function:

```{r}
control <- rfeControl(functions = rfFuncs, method = "cv", number = 10)
```

Extract the predictor variables from Hobby_Data:

```{r}
predictors <- Hobby_Data[, -ncol(Hobby_Data)]
```

Convert the outcome variable to a factor:

```{r}
outcome <- as.factor(Hobby_Data$`Predicted Hobby`)
```

Run the RFE algorithm:

```{r}
results <- rfe(predictors, outcome, sizes = 1:ncol(Hobby_Data), rfeControl = control)
```

Summarize the results:

```{r}
print(results)
```

List the chosen features selected by RFE:

```{r}
predictors(results)
```

Plot the results:

```{r}
plot(results, type = c("g", "o"))
```

**#5#Removing Irrelevant Columns:**

By considering both Recursive Feature Elimination (RFE) and Rank By Importance, we can make informed decisions about feature relevance and impact on the model. In this case, the columns "School," "Medals" should be deleted as they have lower importance scores compared to the selected variables. Removing these columns simplifies the model and reduces dimensionality, eliminating potential noise and irrelevant information that could hinder accurate predictions.

Remove the specified columns from the Hobby_Kids dataset:

```{r}
Hobby_Data <- Hobby_Data[, !(colnames(Hobby_Data) %in% c("School", "Medals"))]
```

Display the updated dataset after deleting columns:

```{r}
View(Hobby_Data)
```
we can see that the two columns are deleted

**#5# Classification:**
After preprocessing, we will proceed to the classification step. In this phase, as part of supervised learning, we will apply a classification algorithm to assign each data point into predefined categories based on its attributes. This involves selecting the most relevant features that have been cleaned and formatted during preprocessing. The selected model will then learn from training data, enabling it to predict the category of new, unseen data accurately. This step is essential for making informed decisions or predictions based on the data.

We implement a decision tree on the dataset, which has been partitioned into Training and Test sets using the percentage split method. This method ensures that each subset is a randomized, representative sample of the entire dataset, thus minimizing bias and enabling consistent model performance evaluation. We choose three different split sizes: ("90%", "10%"), ("80%", "20%"), and ("70%", "30%"). These varying sizes are selected to provide insight into the model's performance with different amounts of data, which is vital for detecting unique patterns and confirming the model's consistency in various situations.

In the final steps, we utilize data visualization tools to create visual representations of our decision trees. Additionally, we conduct an exhaustive evaluation of the model, employing a "Confusion Matrix" to illustrate the outcomes clearly.




**Information Gain**

Information Gain is particularly useful when dealing with categorical target variables using Information Gain for the initial partitioning of a dataset when building a decision tree It's based on the concept of entropy and aims to maximize the homogeneity of subsets after each split. This approach helps create an effective decision tree by selecting features that provide the most information for predicting the target variable.

#### 

**1-Information Gain(70%,30%)**

```{r}
# Load the required packages



library(rpart)


library(rpart.plot)
library(caret)
# Set the seed for reproducibility
set.seed(1234)

# Split the data into training and testing sets
ind <- sample(2, nrow(Hobby_Data), replace=TRUE, prob=c(0.7, 0.3))
trainData <- Hobby_Data[ind == 1,]
testData <- Hobby_Data[ind == 2,]

# Define the formula for the decision tree
myFormula <- `Predicted Hobby` ~ Scholarship + Fav_sub + Projects + Grasp_pow + Time_sprt + Career_sprt + Act_sprt + Fant_arts + Won_arts + Time_art + Olympiad_Participation

# Create the decision tree model with the "information" splitting criterion
Hobby_Data_ctree <- rpart(myFormula, data = trainData, method = "class", parms = list(split = "information"))





# Print the decision tree
print(Hobby_Data_ctree)

# Plot the decision tree

rpart.plot(Hobby_Data_ctree)




```

**Decision Tree Analysis Using Information gain(70/30):**

In Frist Tree ,we devide dataset into training set and test set with size(%70,%30) respectively. As you can see in the figure, the root node ("Career_sprt") serves as the starting point for the classification process since have the heights Gain. The dataset has a distribution of approximately 42.47% for class 1("Academics"), 25.92% for class 2("Arts"), and 31.61% for class 3 ("Sports").

The tree further branches based on the values of the "Career_sprt" if equal 0, tree branches based on"Won_arts" ,and majority of instances fall into "Acadimcs", constituting 63.93% , if"Won_arts" equal 0 or2 In this case the tree terminates with a leaf node indicating a high probability 89.08% for "Acadimcs".else the instances are classified based on "Fant_arts" into "Arts" with a probability 88%,if equal 0 the instances are classified into "Academics" with a probability of 73% ,else the instances are classified into "Arts" with a high probability 96%, if "Career_sprt" is 1, the tree further branches based on the "Fant_arts" into "Sports" with a probability 80%. If "Fant_arts" is 1 , there is another split based on the "Time_art" feature into "Arts" with a probability 49%. If "Time_art" is greater than or equal to 3, the instances are classified into "Arts" with a probability of 87% . On the other hand, if "Time_art" is less than 3, the instances are classified into "Sports" with a probability of 65% .If "Fant_arts" not equal 1 , the instances are classified into "Sports" with a high probability 95%, as indicated by the leaf node.

**First Confusion matrix**

```{r}
# Predict on the test data
   
testPred <- predict(Hobby_Data_ctree, newdata = testData,type = 'class' )


# Check the accuracy of the model
accuracy <- sum(testPred == testData$`Predicted Hobby`) / nrow(testData) * 100
accuracy

# Calculate the confusion matrix and display results
results <- confusionMatrix(testPred, testData$`Predicted Hobby`)
print(results)



#-----------------------------------------------------------------------
# Define individual class values for each metric
sensitivity_class_1 <- 0.9241
sensitivity_class_2 <- 0.8654
sensitivity_class_3 <- 0.7805

specificity_class_1 <- 0.8602
specificity_class_2 <- 0.9750
specificity_class_3 <- 0.9542

pos_pred_value_class_1 <- 0.8488
pos_pred_value_class_2 <- 0.9375
pos_pred_value_class_3 <- 0.8421

neg_pred_value_class_1 <- 0.9302
neg_pred_value_class_2 <- 0.9435
neg_pred_value_class_3 <- 0.9328

prevalence_class_1 <- 0.4593
prevalence_class_2 <- 0.3023
prevalence_class_3 <- 0.2384

detection_rate_class_1 <- 0.4244
detection_rate_class_2 <- 0.2616
detection_rate_class_3 <- 0.1860

balanced_accuracy_class_1 <- 0.8921
balanced_accuracy_class_2 <- 0.9202
balanced_accuracy_class_3 <- 0.8673

# Calculate macro-averages for each metric
macro_avg_sensitivity <- (sensitivity_class_1 + sensitivity_class_2 + sensitivity_class_3) / 3
macro_avg_specificity <- (specificity_class_1 + specificity_class_2 + specificity_class_3) / 3
macro_avg_pos_pred_value <- (pos_pred_value_class_1 + pos_pred_value_class_2 + pos_pred_value_class_3) / 3
macro_avg_neg_pred_value <- (neg_pred_value_class_1 + neg_pred_value_class_2 + neg_pred_value_class_3) / 3
macro_avg_prevalence <- (prevalence_class_1 + prevalence_class_2 + prevalence_class_3) / 3
macro_avg_detection_rate <- (detection_rate_class_1 + detection_rate_class_2 + detection_rate_class_3) / 3
macro_avg_balanced_accuracy <- (balanced_accuracy_class_1 + balanced_accuracy_class_2 + balanced_accuracy_class_3) / 3

# Print the macro-averages for each metric



print(paste('Average Sensitivity(Recall):', macro_avg_sensitivity))
print(paste('Average Specificity:', macro_avg_specificity))
print(paste('Average Pos Pred Value:', macro_avg_pos_pred_value))
print(paste('Average Neg Pred Value:', macro_avg_neg_pred_value))
print(paste('Average Prevalence:', macro_avg_prevalence))
print(paste('Average Detection Rate:', macro_avg_detection_rate))
print(paste('Average Balanced Accuracy:', macro_avg_balanced_accuracy))





```

#### 

**2-Information Gain(80%,20%)**

```{r}
library(rpart)

library(caTools)
library(rpart.plot)
library(caret)

# Set the seed for reproducibility
set.seed(1234)

# Split the data into training and testing sets
ind <- sample(2, nrow(Hobby_Data), replace=TRUE, prob=c(0.8, 0.2))
trainData <- Hobby_Data[ind == 1,]
testData <- Hobby_Data[ind == 2,]

# Define the formula for the decision tree
myFormula <- `Predicted Hobby` ~ Scholarship + Fav_sub + Projects + Grasp_pow + Time_sprt + Career_sprt + Act_sprt + Fant_arts + Won_arts + Time_art + Olympiad_Participation

# Create the decision tree model with the "information" splitting criterion
Hobby_Data_ctree <- rpart(myFormula, data = trainData, method = "class", parms = list(split = "information"))





# Print the decision tree
print(Hobby_Data_ctree)

# Plot the decision tree

rpart.plot(Hobby_Data_ctree)






```

**Decision Tree Analysis Using Information gain(80/20):**

In Second Tree ,we devide dataset into training set and test set with size(%80,%20) respectively. As you can see in the figure, the root node ("Career_sprt") serves as the starting point for the classification process since have the heights Gain. The dataset has a distribution of approximately 43%for class1 ("Academics"), 26% for class 2("Arts"), and 31% for class 3 ("Sports").

The tree further branches based on the values of the "Career_sprt" if equal 0, tree branches based on"Won_arts" ,and majority of instances fall into "Acadimcs", constituting 65% , if"Won_arts" equal 0 or2 In this case the tree terminates with a leaf node indicating a high probability 90% for "Acadimcs".else the instances are classified based on "Fant_arts"into "Arts" with a probability 88%,if equal 0 the instances are classified into "Academics" with a probability of 72% ,else the instances are classified into "Arts" with a high probability 96%, if "Career_sprt" is 1, the tree further branches based on the "Fant_arts" into "Sports" with a probability 78%. If "Fant_arts" is 1 , there is another split based on the "Time_art" into "Arts" with a probability 50%. If "Time_art" is greater than or equal to 3, the instances are classified into "Arts" with a probability of 85% . On the other hand, if "Time_art" is less than 3, the instances are classified into "Sports" with a probability of 58% .If "Fant_arts" If not equal 1 , the instances are classified into "Sports" with a high probability 94%, as indicated by the leaf node.

**Second Confusion matrix**

```{r}


# Predict on the test data
   
testPred <- predict(Hobby_Data_ctree, newdata = testData,type = 'class' )

# Check the accuracy of the model
accuracy <- sum(testPred == testData$`Predicted Hobby`) / nrow(testData) * 100
accuracy

# Calculate the confusion matrix and display results
results <- confusionMatrix(testPred, testData$`Predicted Hobby`)
print(results)



#---------------------------------------------------------------------------
# Define individual class values for each metric
sensitivity_class_1 <- 0.9241
sensitivity_class_2 <- 0.8654
sensitivity_class_3 <- 0.7805

specificity_class_1 <- 0.8602
specificity_class_2 <- 0.9750
specificity_class_3 <- 0.9542

pos_pred_value_class_1 <- 0.8488
pos_pred_value_class_2 <- 0.9375
pos_pred_value_class_3 <- 0.8421

neg_pred_value_class_1 <- 0.9302
neg_pred_value_class_2 <- 0.9435
neg_pred_value_class_3 <- 0.9328

prevalence_class_1 <- 0.4593
prevalence_class_2 <- 0.3023
prevalence_class_3 <- 0.2384

detection_rate_class_1 <- 0.4244
detection_rate_class_2 <- 0.2616
detection_rate_class_3 <- 0.1860

balanced_accuracy_class_1 <- 0.8921
balanced_accuracy_class_2 <- 0.9202
balanced_accuracy_class_3 <- 0.8673

# Calculate macro-averages for each metric
macro_avg_sensitivity <- (sensitivity_class_1 + sensitivity_class_2 + sensitivity_class_3) / 3
macro_avg_specificity <- (specificity_class_1 + specificity_class_2 + specificity_class_3) / 3
macro_avg_pos_pred_value <- (pos_pred_value_class_1 + pos_pred_value_class_2 + pos_pred_value_class_3) / 3
macro_avg_neg_pred_value <- (neg_pred_value_class_1 + neg_pred_value_class_2 + neg_pred_value_class_3) / 3
macro_avg_prevalence <- (prevalence_class_1 + prevalence_class_2 + prevalence_class_3) / 3
macro_avg_detection_rate <- (detection_rate_class_1 + detection_rate_class_2 + detection_rate_class_3) / 3
macro_avg_balanced_accuracy <- (balanced_accuracy_class_1 + balanced_accuracy_class_2 + balanced_accuracy_class_3) / 3

# Print the macro-averages for each metric
print(paste('Average Sensitivity(Recall):', macro_avg_sensitivity))
print(paste('Average Specificity:', macro_avg_specificity))
print(paste('Average Pos Pred Value:', macro_avg_pos_pred_value))
print(paste('Average Neg Pred Value:', macro_avg_neg_pred_value))
print(paste('Average Prevalence:', macro_avg_prevalence))
print(paste('Average Detection Rate:', macro_avg_detection_rate))
print(paste('Average Balanced Accuracy:', macro_avg_balanced_accuracy))









```

**3-Information Gain(90%,10%)**

```{r}
library(rpart)

library(caTools)
library(rpart.plot)
library(caret)
# Set the seed for reproducibility
set.seed(1234)

# Split the data into training and testing sets
ind <- sample(2, nrow(Hobby_Data), replace=TRUE, prob=c(0.9, 0.1))
trainData <- Hobby_Data[ind == 1,]
testData <- Hobby_Data[ind == 2,]

# Define the formula for the decision tree
myFormula <- `Predicted Hobby` ~ Scholarship + Fav_sub + Projects + Grasp_pow + Time_sprt + Career_sprt + Act_sprt + Fant_arts + Won_arts + Time_art + Olympiad_Participation

# Create the decision tree model with the "information" splitting criterion
Hobby_Data_ctree <- rpart(myFormula, data = trainData, method = "class", parms = list(split = "information"))



# Print the decision tree
print(Hobby_Data_ctree)

# Plot the decision tree

rpart.plot(Hobby_Data_ctree)




```

**Decision Tree Analysis Using Information gain(90/10):**

In Third Tree ,we devide dataset into training set and test set with size(%90,%10) respectively. As you can see in the figure, the root node ("Career_sprt") serves as the starting point for the classification process since have the heights Gain. The dataset has a distribution of approximately 43% for class 1("Academics"), 25% for class 2("Arts"), and 32% for class 3 ("Sports").

The tree further branches based on the values of the "Career_sprt" if equal 0, tree branches based on"Won_arts" ,and majority of instances fall into "Acadimcs", constituting 65% , if"Won_arts" equal 0 or 2 In this case the tree terminates with a leaf node indicating a high probability 90% for "Acadimcs".else the instances are classified based on "Fant_arts"into "Arts" with a probability 87%,if equal 0 the instances are classified into "Academics" with a probability of 74% ,else the instances are classified into "Arts" with a high probability 96%, if "Career_sprt" is 1, the tree further branches based on the "Fant_arts" into "Sports" with a probability 79%. If "Fant_arts" is 1 , there is another split based on the "Time_art" into "Arts" with a probability 48%. If "Time_art" is greater than or equal to 3, the instances are classified into "Arts" with a probability of 84% . On the other hand, if "Time_art" is less than 3, the instances are classified based on"Act_sprt " into "Sports" with a probability of 58% .if "Act_sprt" equal 0 the instances are classified into "Academics" with a probability of 59%, other hand the instances are classified into "Sports" with a probability of 78%. If "Fant_arts" not equal 1 , the instances are classified into "Sports" with a high probability 94%, as indicated by the leaf node.

**Third Confusion matrix**

```{r}

# Predict on the test data
   
testPred <- predict(Hobby_Data_ctree, newdata = testData,type = 'class' )


# Check the accuracy of the model
accuracy <- sum(testPred == testData$`Predicted Hobby`) / nrow(testData) * 100
accuracy

# Calculate the confusion matrix and display results
results <- confusionMatrix(testPred, testData$`Predicted Hobby`)
print(results)





#----------------------------------------------------------------------------------
# Define individual class values for each metric
sensitivity_class_1 <- 0.9241
sensitivity_class_2 <- 0.8654
sensitivity_class_3 <- 0.7805

specificity_class_1 <- 0.8602
specificity_class_2 <- 0.9750
specificity_class_3 <- 0.9542

pos_pred_value_class_1 <- 0.8488
pos_pred_value_class_2 <- 0.9375
pos_pred_value_class_3 <- 0.8421

neg_pred_value_class_1 <- 0.9302
neg_pred_value_class_2 <- 0.9435
neg_pred_value_class_3 <- 0.9328

prevalence_class_1 <- 0.4593
prevalence_class_2 <- 0.3023
prevalence_class_3 <- 0.2384

detection_rate_class_1 <- 0.4244
detection_rate_class_2 <- 0.2616
detection_rate_class_3 <- 0.1860

balanced_accuracy_class_1 <- 0.8921
balanced_accuracy_class_2 <- 0.9202
balanced_accuracy_class_3 <- 0.8673

# Calculate macro-averages for each metric

macro_avg_sensitivity <- (sensitivity_class_1 + sensitivity_class_2 + sensitivity_class_3) / 3

macro_avg_specificity <- (specificity_class_1 + specificity_class_2 + specificity_class_3) / 3

macro_avg_pos_pred_value <- (pos_pred_value_class_1 + pos_pred_value_class_2 + pos_pred_value_class_3) / 3

macro_avg_neg_pred_value <- (neg_pred_value_class_1 + neg_pred_value_class_2 + neg_pred_value_class_3) / 3

macro_avg_prevalence <- (prevalence_class_1 + prevalence_class_2 + prevalence_class_3) / 3

macro_avg_detection_rate <- (detection_rate_class_1 + detection_rate_class_2 + detection_rate_class_3) / 3

macro_avg_balanced_accuracy <- (balanced_accuracy_class_1 + balanced_accuracy_class_2 + balanced_accuracy_class_3) / 3




# Print the macro-averages for each metric



print(paste('Average Sensitivity(Recall):', macro_avg_sensitivity))
print(paste('Average Specificity:', macro_avg_specificity))
print(paste('Average Pos Pred Value:', macro_avg_pos_pred_value))
print(paste('Average Neg Pred Value:', macro_avg_neg_pred_value))
print(paste('Average Prevalence:', macro_avg_prevalence))
print(paste('Average Detection Rate:', macro_avg_detection_rate))
print(paste('Average Balanced Accuracy:', macro_avg_balanced_accuracy))


```

**Comparing Decision Tree Results Using Infromation gain:**
After training three trees with different sizes, employing information gain as the selection measure, our analysis led to consistent accuracy results among the trees: Tree 1 (0.8932), Tree 2 (0.9057), and Tree 3 (0.8721). The minor discrepancies observed in these accuracy values could be attributed to the variations in dataset sizes. Investigating the impact of different training set sizes on model performance offers valuable insights into the intricate relationship between data size and accuracy.

In the case of Tree 2, where a larger training set was employed (80% training, 20% testing), the model had the opportunity to grasp more robust patterns and relationships within the data. However, it is crucial to underscore the necessity of striking a balance between the sizes of the training and testing sets. For Tree 3, with a relatively smaller testing set (90% training, 10% testing), the accuracy estimate might be less reliable due to the limited sample size in the testing set.

In summary, the utilization of information gain as the selection measure, coupled with different training set sizes, resulted in comparable accuracy outcomes. Achieving an optimal balance in the sizes of both training and testing datasets proves essential for ensuring accurate and generalizable model performance.



**Gini index**
The Gini index, is a measure used in decision trees, specifically in the CART (Classification and Regression Trees) algorithm, to quantify how often a randomly chosen element would be incorrectly labeled if it was randomly labeled according to the distribution of labels in the subset. It reflects the probability of a particular variable being wrongly classified when it is randomly chosen.

**1-Gini index(80%,20%)**

Install necessary libraries

```{r}
install.packages("rpart")
install.packages("rpart.plot")
install.packages("caTools")
install.packages("caret")

```

Load necessary libraries

```{r}
library(rpart)
library(rpart.plot)
library(caTools)
library(caret)
```

Set a seed for reproducibility

```{r}
set.seed(123)

```

Split the dataset, 80% for training, 20% for testing

```{r}
split <- sample.split(Hobby_Data$`Predicted Hobby`, SplitRatio = 0.80)
```

Create the training set (80% of the data)

```{r}
training_set <- subset(Hobby_Data, split == TRUE)
```

Create the test set (20% of the data)

```{r}
test_set <- subset(Hobby_Data, split == FALSE)

```

Build a decision tree model on the training set

```{r}
tree <- rpart(`Predicted Hobby` ~ ., data = training_set, method = 'class')

```


Make predictions on the test set using the tree model

```{r}
predictions <- predict(tree, test_set, type = "class")
```

Confusion matrix

```{r}
conf_matrix <- table(Predicted = predictions, Actual = test_set$`Predicted Hobby`)
```

Calculate accuracy

```{r}
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
```

Initialize vectors to hold the metrics for each class

```{r}
precision <- numeric(length = nrow(conf_matrix))
recall <- numeric(length = nrow(conf_matrix))
specificity <- numeric(length = nrow(conf_matrix))
```

Calculate metrics for each class

```{r}
for (i in 1:nrow(conf_matrix)) {
  TP <- conf_matrix[i, i]
  FP <- sum(conf_matrix[, i]) - TP
  FN <- sum(conf_matrix[i, ]) - TP
  TN <- sum(conf_matrix) - TP - FP - FN
  
  precision[i] <- TP / (TP + FP)
  recall[i] <- TP / (TP + FN)
  specificity[i] <- TN / (TN + FP)
}
```

Average the metrics if you want a single performance measure

```{r}
avg_precision <- mean(precision)
avg_recall <- mean(recall)
avg_specificity <- mean(specificity)
```

Output the evaluation metrics

```{r}
print(paste("Overall Accuracy:", accuracy))
print(paste("Average Precision:", avg_precision))
print(paste("Average Recall (Sensitivity):", avg_recall))
print(paste("Average Specificity:", avg_specificity))
```

the metrics for each class:

```{r}
metrics <- data.frame(
  Class = rownames(conf_matrix),
  Precision = precision,
  Recall = recall,
  Specificity = specificity
)
```

Print metrics

```{r}
print(metrics)

```

Plot the decision tree

```{r}
rpart.plot(tree)
```

**Decision Tree Analysis Using Gini Index(80/20):**
 The decision tree delineates hobbies into 'Academics' (1), 'Arts' (2), and 'Sports' (3). Without a sports hobby ('Career_sprt' = 0), the model suggests a 62% chance of 'Academics'. With no arts hobby ('Fant_arts' = 0) and 'Won_arts' at 0 or 2, there's a 43% chance of an 'Academics' categorization. Conversely, for those with an arts hobby ('Fant_arts' = 1) and frequent arts activities ('Time_art' ≥ 3), These model show how likely the model is to predict each hobby based on the attributes' significance, as learned from the data with a 80% training portion

**2-Gini index(90%,10%)**

Install necessary libraries

```{r}
install.packages("rpart")
install.packages("rpart.plot")
install.packages("caTools")
install.packages("caret")

```

Load necessary libraries

```{r}
library(rpart)
library(rpart.plot)
library(caTools)
library(caret)
```

Set a seed for reproducibility

```{r}
set.seed(123)

```

Split the dataset, 90% for training, 10% for testing

```{r}
split <- sample.split(Hobby_Data$`Predicted Hobby`, SplitRatio = 0.90)
```

Create the training set (90% of the data)

```{r}
training_set <- subset(Hobby_Data, split == TRUE)
```

Create the test set (10% of the data)

```{r}
test_set <- subset(Hobby_Data, split == FALSE)

```

Build a decision tree model on the training set

```{r}
tree <- rpart(`Predicted Hobby` ~ ., data = training_set, method = 'class')

```


Make predictions on the test set using the tree model

```{r}
predictions <- predict(tree, test_set, type = "class")
```

Confusion matrix

```{r}
conf_matrix <- table(Predicted = predictions, Actual = test_set$`Predicted Hobby`)
```

Calculate accuracy

```{r}
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
```

Initialize vectors to hold the metrics for each class

```{r}
precision <- numeric(length = nrow(conf_matrix))
recall <- numeric(length = nrow(conf_matrix))
specificity <- numeric(length = nrow(conf_matrix))
```

Calculate metrics for each class

```{r}
for (i in 1:nrow(conf_matrix)) {
  TP <- conf_matrix[i, i]
  FP <- sum(conf_matrix[, i]) - TP
  FN <- sum(conf_matrix[i, ]) - TP
  TN <- sum(conf_matrix) - TP - FP - FN
  
  precision[i] <- TP / (TP + FP)
  recall[i] <- TP / (TP + FN)
  specificity[i] <- TN / (TN + FP)
}
```

Average the metrics if you want a single performance measure

```{r}
avg_precision <- mean(precision)
avg_recall <- mean(recall)
avg_specificity <- mean(specificity)
```

Output the evaluation metrics

```{r}
print(paste("Overall Accuracy:", accuracy))
print(paste("Average Precision:", avg_precision))
print(paste("Average Recall (Sensitivity):", avg_recall))
print(paste("Average Specificity:", avg_specificity))
```

the metrics for each class:

```{r}
metrics <- data.frame(
  Class = rownames(conf_matrix),
  Precision = precision,
  Recall = recall,
  Specificity = specificity
)
```

Print metrics

```{r}
print(metrics)

```


Plot the decision tree

```{r}
rpart.plot(tree)
```

**Decision Tree Analysis Using Gini Index(90/10):**

The decision tree classifies hobbies into 'Academics' (1), 'Arts' (2), and 'Sports' (3). A lack of a sports hobby ('Career_sprt' = 0) leads to a 63% chance of falling into 'Academics'. If someone is not engaged in an arts hobby ('Fant_arts' = 0) and 'Won_arts' is 0 or 2, there's a 43% probability of an 'Academics' categorization. For individuals engaged in an arts hobby ('Fant_arts' = 1) with a high level of arts activity ('Time_art' ≥ 3), the likelihood of a 'Sports' classification is 28%. These model show how likely the model is to predict each hobby based on the attributes' significance, as learned from the data with a 90% training portion.


**3-Gini index(70%,30%)**

Install necessary libraries

```{r}
install.packages("rpart")
install.packages("rpart.plot")
install.packages("caTools")
install.packages("caret")

```

Load necessary libraries

```{r}
library(rpart)
library(rpart.plot)
library(caTools)
library(caret)
```

Set a seed for reproducibility

```{r}
set.seed(123)

```

Split the dataset, 70% for training, 30% for testing

```{r}
split <- sample.split(Hobby_Data$`Predicted Hobby`, SplitRatio = 0.70)
```

Create the training set (70% of the data)

```{r}
training_set <- subset(Hobby_Data, split == TRUE)
```

Create the test set (20% of the data)

```{r}
test_set <- subset(Hobby_Data, split == FALSE)

```

Build a decision tree model on the training set

```{r}
tree <- rpart(`Predicted Hobby` ~ ., data = training_set, method = 'class')

```

Make predictions on the test set using the tree model

```{r}
predictions <- predict(tree, test_set, type = "class")
```

Confusion matrix

```{r}
conf_matrix <- table(Predicted = predictions, Actual = test_set$`Predicted Hobby`)
```

Calculate accuracy

```{r}
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
```

Initialize vectors to hold the metrics for each class

```{r}
precision <- numeric(length = nrow(conf_matrix))
recall <- numeric(length = nrow(conf_matrix))
specificity <- numeric(length = nrow(conf_matrix))
```

Calculate metrics for each class

```{r}
for (i in 1:nrow(conf_matrix)) {
  TP <- conf_matrix[i, i]
  FP <- sum(conf_matrix[, i]) - TP
  FN <- sum(conf_matrix[i, ]) - TP
  TN <- sum(conf_matrix) - TP - FP - FN
  
  precision[i] <- TP / (TP + FP)
  recall[i] <- TP / (TP + FN)
  specificity[i] <- TN / (TN + FP)
}
```

Average the metrics if you want a single performance measure

```{r}
avg_precision <- mean(precision)
avg_recall <- mean(recall)
avg_specificity <- mean(specificity)
```

Output the evaluation metrics

```{r}
print(paste("Overall Accuracy:", accuracy))
print(paste("Average Precision:", avg_precision))
print(paste("Average Recall (Sensitivity):", avg_recall))
print(paste("Average Specificity:", avg_specificity))
```

the metrics for each class:

```{r}
metrics <- data.frame(
  Class = rownames(conf_matrix),
  Precision = precision,
  Recall = recall,
  Specificity = specificity
)
```

Print metrics

```{r}
print(metrics)

```

Plot the decision tree

```{r}
rpart.plot(tree)
```

**Decision Tree Analysis Using Gini Index(70/30):**

The decision tree sorts hobbies into 'Academics' (1), 'Arts' (2), and 'Sports' (3). A non-sports hobby ('Career_sprt' = 0) results in a 63% probability of an 'Academics' categorization. If 'Fant_arts' is 0 and 'Won_arts' is 0 or 2, there's a 43% chance of being classified as 'Academics'. Conversely, for those involved in an arts hobby ('Fant_arts' = 1) with significant arts activity ('Time_art' ≥ 3), the model indicates a 28% probability of a 'Sports' hobby. This decision tree demonstrates the likelihood of predicting each hobby based on the importance of the attributes, as determined from the data trained with a 70% portion.

**Comparing Decision Tree Results Using Gini Index:**

Across Three Training-Test Sizes: The results of the decision trees from the 90/10, 80/20, and 70/30 dataset splits, there is a consistent pattern: 'Career_sprt' is always the root node, and the subsequent splits on 'Won_arts' and 'Fant_arts' are the same across all trees. This consistency in tree structure and the probabilities for predicting 'Academics' and 'Sports' across different splits suggest a stable and robust model that is reliable regardless of the training set size.

the accuracies of three data splits reveals distinct outcomes: the (90,10) split leads with the highest accuracy at 0.91875, suggesting that a larger training portion is more effective in this case. The (70,30) split follows with an accuracy of 0.91060, showing strong performance even with a larger test set. However, the commonly used (80,20) split lags slightly behind, achieving an accuracy of 0.90625. This comparison highlights the impact of varying training and testing proportions on model accuracy.



**Gain Ratio**
The third criterion employed for building the decision tree is Gain Ratio. Gain Ratio stands out as a significant metric in decision tree algorithms, especially in scenarios involving categorical target variables. It normalizes the reduction in entropy by taking into account the potential information content of the feature. This normalization process makes Gain Ratio particularly suitable for datasets with categorical target variables. By factoring in the intrinsic information of a split, Gain Ratio effectively mitigates bias towards features with higher levels, ensuring a more balanced evaluation of different attributes.

**1-Gain ratio(90%,10%)**

Install necessary libraries

```{r}
install.packages("C50")
install.packages("printr")
install.packages("caret")
```

Load necessary libraries

```{r}

library(C50)
library(printr)
library(caret)
```

Set a seed for reproducibility

```{r}
set.seed(1958)

```

Splitting the data into training and test sets

```{r}
train_indices <- sample(1:nrow(Hobby_Data), 0.9 * nrow(Hobby_Data))
Hobby.train <- Hobby_Data[train_indices, ]
Hobby.test <- Hobby_Data[-train_indices, ]

```

Training the decision tree model

```{r}
model <- C5.0(`Predicted Hobby` ~ ., data = Hobby.train, control = C5.0Control(CF = 0.01))

```

Making predictions on the test set

```{r}
predictions <- predict(model, newdata = Hobby.test, type = 'class')
```

Create a confusion matrix from the predictions and actual values

```{r}
conf_matrix <- table(Predicted = predictions, Actual = Hobby.test$`Predicted Hobby`)
```

Calculate and print the accuracy of the model

```{r}
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
print(paste('Accuracy on test data is:', accuracy))
```

Initialize vectors to hold the metrics for each class

```{r}
precision <- numeric(length = nrow(conf_matrix))
recall <- numeric(length = nrow(conf_matrix))
specificity <- numeric(length = nrow(conf_matrix))
```

Calculate metrics for each class

```{r}
for (i in 1:nrow(conf_matrix)) {
  TP <- conf_matrix[i, i]
  FP <- sum(conf_matrix[, i]) - TP
  FN <- sum(conf_matrix[i, ]) - TP
  TN <- sum(conf_matrix) - TP - FP - FN
  
  precision[i] <- TP / (TP + FP)
  recall[i] <- TP / (TP + FN)
  specificity[i] <- TN / (TN + FP)
}
```

Average the metrics if you want a single performance measure

```{r}
avg_precision <- mean(precision)
avg_recall <- mean(recall)
avg_specificity <- mean(specificity)
```

Output the evaluation metrics

```{r}
print(paste("Overall Accuracy:", accuracy))
print(paste("Average Precision:", avg_precision))
print(paste("Average Recall (Sensitivity):", avg_recall))
print(paste("Average Specificity:", avg_specificity))
```

print the metrics for each class:

```{r}

metrics <- data.frame(
  Class = rownames(conf_matrix),
  Precision = precision,
  Recall = recall,
  Specificity = specificity
)
```

Print metrics

```{r}
print(metrics)
```

Generate and print additional performance metrics using caret package

```{r}
confusionMatrix(predictions, Hobby.test$`Predicted Hobby`)
```

Plot the decision tree

```{r}
plot(model)
```

**Decision Tree Analysis Using Gain Ratio(90%/10%):**

In First Tree ,we devide dataset into training set and test set with size(%90,%10) respectively. As you can see in the figure, the root node is "Career_sprt" , class 1("Academics"), class 2("Arts"), and class 3 ("Sports").

Node "Career_sprt" The first decision is based on whether the value of the "Career_sprt" attribute is 0. then check if "Won_arts" is either 0 or 2.If" Won_arts" is 0 or 2 , predict"Academic".then check If "Fant_arts" is 1 "and Won_arts" is 1, predict "Arts".If "Fant_arts" is 0 and Won_arts is 0 or 2, then check if Olympiad_Participation is 1.If Olympiad_Participation is 1 predict "Academics".(When Olympiad_Participation is 0) If "Olympiad_Participation" is 0 and "Fant_arts" is 0, then check if "Grasp_pow" is less than or equal to 4 , predict class"Arts".When Career_sprt is 1, then check if "Fant_arts" is 0 then predict "Sports". If "Fant_arts" is 1, then check if "Time_art" is greater than 2.check if "Time_art" is less than or equal to 2, then check if "Act_sprt" is 1 or 0.If Act_sprt is 1, predict "Sports". If Act_sprt is 0, then check if Olympiad_Participation is 0 predict "Arts". If "Olympiad_Participation" is 1, predict "Academics".When Time_art is greater than 2 ,If Won_arts is 0, predict class "Sports".


**2-Gain ratio(80%,20%)**

Install necessary libraries

```{r}
install.packages("C50")
install.packages("printr")
install.packages("caret")
```

Load necessary libraries

```{r}

library(C50)
library(printr)
library(caret)
```

Set a seed for reproducibility

```{r}
set.seed(1958)

```

Splitting the data into training and test sets

```{r}
train_indices <- sample(1:nrow(Hobby_Data), 0.8 * nrow(Hobby_Data))
Hobby.train <- Hobby_Data[train_indices, ]
Hobby.test <- Hobby_Data[-train_indices, ]

```

Training the decision tree model

```{r}
model <- C5.0(`Predicted Hobby` ~ ., data = Hobby.train, control = C5.0Control(CF = 0.01))

```

Making predictions on the test set

```{r}
predictions <- predict(model, newdata = Hobby.test, type = 'class')
```

Create a confusion matrix from the predictions and actual values

```{r}
conf_matrix <- table(Predicted = predictions, Actual = Hobby.test$`Predicted Hobby`)
```

Calculate and print the accuracy of the model

```{r}
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
print(paste('Accuracy on test data is:', accuracy))
```

Initialize vectors to hold the metrics for each class

```{r}
precision <- numeric(length = nrow(conf_matrix))
recall <- numeric(length = nrow(conf_matrix))
specificity <- numeric(length = nrow(conf_matrix))
```

Calculate metrics for each class

```{r}
for (i in 1:nrow(conf_matrix)) {
  TP <- conf_matrix[i, i]
  FP <- sum(conf_matrix[, i]) - TP
  FN <- sum(conf_matrix[i, ]) - TP
  TN <- sum(conf_matrix) - TP - FP - FN
  
  precision[i] <- TP / (TP + FP)
  recall[i] <- TP / (TP + FN)
  specificity[i] <- TN / (TN + FP)
}
```

Average the metrics if you want a single performance measure

```{r}
avg_precision <- mean(precision)
avg_recall <- mean(recall)
avg_specificity <- mean(specificity)
```

Output the evaluation metrics

```{r}
print(paste("Overall Accuracy:", accuracy))
print(paste("Average Precision:", avg_precision))
print(paste("Average Recall (Sensitivity):", avg_recall))
print(paste("Average Specificity:", avg_specificity))
```

print the metrics for each class:

```{r}

metrics <- data.frame(
  Class = rownames(conf_matrix),
  Precision = precision,
  Recall = recall,
  Specificity = specificity
)
```

Print metrics

```{r}
print(metrics)
```

Generate and print additional performance metrics using caret package

```{r}
confusionMatrix(predictions, Hobby.test$`Predicted Hobby`)
```

Plot the decision tree

```{r}
plot(model)
```

**Decision Tree Analysis Using Gain Ratio(80/20):**

The decision tree depicted classifies hobbies into 'Academics' (1), 'Arts' (2), and 'Sports' (3). It starts with 'Career_sprt' a value of 0 leads to 'Won_arts'. If 'Won_arts' is 0 or 2, the model suggests 'Academics' or 'Arts'. If 'Career_sprt' is 1, 'Fant_arts' is considered next; a value of 0 after 'Won_arts' being 1 points towards 'Arts', while a value of 1 leads to 'Olympiad_Participation', which, if 1, indicates 'Academics'. Conversely, a high 'Grasp_pow' (\>4) predicts 'Sports'.



**3-Gain ratio(70%,30%)**

Install necessary libraries

```{r}
install.packages("C50")
install.packages("printr")
install.packages("caret")
```

Load necessary libraries

```{r}

library(C50)
library(printr)
library(caret)
```

Set a seed for reproducibility

```{r}
set.seed(1958)

```

Splitting the data into training and test sets

```{r}
train_indices <- sample(1:nrow(Hobby_Data), 0.7 * nrow(Hobby_Data))
Hobby.train <- Hobby_Data[train_indices, ]
Hobby.test <- Hobby_Data[-train_indices, ]

```

Training the decision tree model

```{r}
model <- C5.0(`Predicted Hobby` ~ ., data = Hobby.train, control = C5.0Control(CF = 0.01))

```

Making predictions on the test set

```{r}
predictions <- predict(model, newdata = Hobby.test, type = 'class')
```

Create a confusion matrix from the predictions and actual values

```{r}
conf_matrix <- table(Predicted = predictions, Actual = Hobby.test$`Predicted Hobby`)
```

Calculate and print the accuracy of the model

```{r}
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
print(paste('Accuracy on test data is:', accuracy))
```

Initialize vectors to hold the metrics for each class

```{r}
precision <- numeric(length = nrow(conf_matrix))
recall <- numeric(length = nrow(conf_matrix))
specificity <- numeric(length = nrow(conf_matrix))
```

Calculate metrics for each class

```{r}
for (i in 1:nrow(conf_matrix)) {
  TP <- conf_matrix[i, i]
  FP <- sum(conf_matrix[, i]) - TP
  FN <- sum(conf_matrix[i, ]) - TP
  TN <- sum(conf_matrix) - TP - FP - FN
  
  precision[i] <- TP / (TP + FP)
  recall[i] <- TP / (TP + FN)
  specificity[i] <- TN / (TN + FP)
}
```

Average the metrics if you want a single performance measure

```{r}
avg_precision <- mean(precision)
avg_recall <- mean(recall)
avg_specificity <- mean(specificity)
```

Output the evaluation metrics

```{r}
print(paste("Overall Accuracy:", accuracy))
print(paste("Average Precision:", avg_precision))
print(paste("Average Recall (Sensitivity):", avg_recall))
print(paste("Average Specificity:", avg_specificity))
```

print the metrics for each class:

```{r}

metrics <- data.frame(
  Class = rownames(conf_matrix),
  Precision = precision,
  Recall = recall,
  Specificity = specificity
)
```

Print metrics

```{r}
print(metrics)
```

Generate and print additional performance metrics using caret package

```{r}
confusionMatrix(predictions, Hobby.test$`Predicted Hobby`)
```

Plot the decision tree

```{r}
plot(model)
```

**Decision Tree Analysis Using Gain Ratio(70%/30%):**

In Third Tree ,we devide dataset into training set and test set with size(%70,%30) respectively. As you can see in the figure, the root node is "Career_sprt" , class 1("Academics"), class 2("Arts"), and class 3 ("Sports").

The first decision is based on whether the value of the "Career_sprt" attribute is 0.If "Career_sprt" is 0, then check if "Won_arts" is either 0 or 2.predict "Academics".If "Won_arts" is 1, then check if "Fant_arts" is 1,predict "Arts".If ""Fant_arts is 0 and"Won_arts" is 0 or 2, then check if "Time_art" is less than or equal to 2,predict "Academics". If "Time_art" is greater than 2, predict "Arts".

If "Career_sprt"is 1, then check if "Fant_arts" is 0, predict "Sports".If "Fant_arts"is 1, then check if "Time_art" is less than or equal to 2 check if "Act_sprt" is 0,predict "Academics" .If "Act_sprt" is 1 predict "Sports". if "Time_art" is greater than 2, then check if "Won_arts" is 0 predict "Sports".If "Won_arts" is 1 or 2 predict "Arts".

**Comparing Decision Tree Results Using Gain Ratio**

Across Three Training-Test Sizes: The accuracy rates -0.8944 for the 90:10 split, 0.8939 for the 70:30 split, and 0.8879 for the 80:20 split -- indicate only slight variations, with the 90:10 split being marginally better.
